{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for functions.py, Text.py and Chain_class.py x:\n",
    "## https://github.com/klaudia-nazarko/nlg-text-generation \n",
    "## commit: b7a35b9d9112305408cb284f5dfc0a91b43c5829\n",
    "\n",
    "\n",
    "import functions as f\n",
    "from Text import * \n",
    "from Chain_class import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#article names\n",
    "articles= ['The_Beatles','Led_Zeppelin','The_Rolling_Stones','Black_Sabbath','Fluorine','Chlorine','Bromine','Iodine']\n",
    "\n",
    "#path to dataset file\n",
    "path = './data/input/'+ article +'_depth_5_spread_10'\n",
    "\n",
    "#open pickle\n",
    "with open(path, \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12063"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_string = [0,0,0,0,0]\n",
    "depth = 1\n",
    "wiki_string[0] = \" \\n\".join(list(dataset[1].values()))\n",
    "length = int(len(Text(\" \\n\".join(wiki_string[0:1])).content.split())/9)    #average length of d=1 summaries\n",
    "wiki_string[1] = \" \\n\".join(list(dataset[2].values()))\n",
    "wiki_string[2] = \" \\n\".join(list(dataset[3].values()))\n",
    "wiki_string[3] = \" \\n\".join(list(dataset[4].values()))\n",
    "wiki_string[4] = \" \\n\".join(list(dataset[5].values()))\n",
    "\n",
    "#parse text using Text function\n",
    "wiki_text = Text(\" \\n\".join(wiki_string[0:depth]))\n",
    "\n",
    "#len(wiki_text.content) #output word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 2282, distinct tokens: 904\n",
      "ngrams level: 3, total ngrams: 2280, distinct ngrams: 2174\n",
      "total tokens: 20319, distinct tokens: 4753\n",
      "ngrams level: 3, total ngrams: 20317, distinct ngrams: 18701\n",
      "total tokens: 192731, distinct tokens: 20493\n",
      "ngrams level: 3, total ngrams: 192729, distinct ngrams: 157754\n",
      "total tokens: 1046286, distinct tokens: 58217\n",
      "ngrams level: 3, total ngrams: 1046284, distinct ngrams: 753635\n",
      "total tokens: 10760274, distinct tokens: 253494\n",
      "ngrams level: 3, total ngrams: 10760272, distinct ngrams: 6064790\n",
      "total tokens: 2384, distinct tokens: 927\n",
      "ngrams level: 3, total ngrams: 2382, distinct ngrams: 2234\n",
      "total tokens: 19918, distinct tokens: 4292\n",
      "ngrams level: 3, total ngrams: 19916, distinct ngrams: 17773\n",
      "total tokens: 166399, distinct tokens: 17245\n",
      "ngrams level: 3, total ngrams: 166397, distinct ngrams: 133562\n",
      "total tokens: 1016375, distinct tokens: 52900\n",
      "ngrams level: 3, total ngrams: 1016373, distinct ngrams: 714340\n",
      "total tokens: 10571597, distinct tokens: 229227\n",
      "ngrams level: 3, total ngrams: 10571595, distinct ngrams: 5795099\n",
      "total tokens: 3320, distinct tokens: 1193\n",
      "ngrams level: 3, total ngrams: 3318, distinct ngrams: 3155\n",
      "total tokens: 21345, distinct tokens: 4863\n",
      "ngrams level: 3, total ngrams: 21343, distinct ngrams: 19520\n",
      "total tokens: 168793, distinct tokens: 18351\n",
      "ngrams level: 3, total ngrams: 168791, distinct ngrams: 136837\n",
      "total tokens: 1058986, distinct tokens: 56565\n",
      "ngrams level: 3, total ngrams: 1058984, distinct ngrams: 742667\n",
      "total tokens: 10887858, distinct tokens: 247610\n",
      "ngrams level: 3, total ngrams: 10887856, distinct ngrams: 6012726\n",
      "total tokens: 3827, distinct tokens: 1308\n",
      "ngrams level: 3, total ngrams: 3825, distinct ngrams: 3615\n",
      "total tokens: 18638, distinct tokens: 4338\n",
      "ngrams level: 3, total ngrams: 18636, distinct ngrams: 17011\n",
      "total tokens: 169222, distinct tokens: 17883\n",
      "ngrams level: 3, total ngrams: 169220, distinct ngrams: 136033\n",
      "total tokens: 997773, distinct tokens: 55003\n",
      "ngrams level: 3, total ngrams: 997771, distinct ngrams: 707919\n",
      "total tokens: 10861958, distinct tokens: 253206\n",
      "ngrams level: 3, total ngrams: 10861956, distinct ngrams: 6075951\n",
      "total tokens: 2246, distinct tokens: 900\n",
      "ngrams level: 3, total ngrams: 2244, distinct ngrams: 2186\n",
      "total tokens: 18536, distinct tokens: 4341\n",
      "ngrams level: 3, total ngrams: 18534, distinct ngrams: 17250\n",
      "total tokens: 174669, distinct tokens: 19685\n",
      "ngrams level: 3, total ngrams: 174667, distinct ngrams: 148123\n",
      "total tokens: 1044248, distinct tokens: 62320\n",
      "ngrams level: 3, total ngrams: 1044246, distinct ngrams: 788283\n",
      "total tokens: 11463734, distinct tokens: 318553\n",
      "ngrams level: 3, total ngrams: 11463732, distinct ngrams: 6884772\n",
      "total tokens: 1864, distinct tokens: 723\n",
      "ngrams level: 3, total ngrams: 1862, distinct ngrams: 1780\n",
      "total tokens: 20269, distinct tokens: 4484\n",
      "ngrams level: 3, total ngrams: 20267, distinct ngrams: 18602\n",
      "total tokens: 175345, distinct tokens: 18197\n",
      "ngrams level: 3, total ngrams: 175343, distinct ngrams: 144578\n",
      "total tokens: 1040326, distinct tokens: 58564\n",
      "ngrams level: 3, total ngrams: 1040324, distinct ngrams: 770061\n",
      "total tokens: 11389379, distinct tokens: 302496\n",
      "ngrams level: 3, total ngrams: 11389377, distinct ngrams: 6765685\n",
      "total tokens: 2337, distinct tokens: 811\n",
      "ngrams level: 3, total ngrams: 2335, distinct ngrams: 2185\n",
      "total tokens: 21268, distinct tokens: 4255\n",
      "ngrams level: 3, total ngrams: 21266, distinct ngrams: 19149\n",
      "total tokens: 176617, distinct tokens: 17274\n",
      "ngrams level: 3, total ngrams: 176615, distinct ngrams: 144411\n",
      "total tokens: 1027849, distinct tokens: 56636\n",
      "ngrams level: 3, total ngrams: 1027847, distinct ngrams: 758889\n",
      "total tokens: 11739156, distinct tokens: 291491\n",
      "ngrams level: 3, total ngrams: 11739154, distinct ngrams: 6659513\n",
      "total tokens: 2779, distinct tokens: 905\n",
      "ngrams level: 3, total ngrams: 2777, distinct ngrams: 2616\n",
      "total tokens: 25881, distinct tokens: 4739\n",
      "ngrams level: 3, total ngrams: 25879, distinct ngrams: 23142\n",
      "total tokens: 197719, distinct tokens: 19327\n",
      "ngrams level: 3, total ngrams: 197717, distinct ngrams: 162208\n",
      "total tokens: 1114357, distinct tokens: 60472\n",
      "ngrams level: 3, total ngrams: 1114355, distinct ngrams: 822465\n",
      "total tokens: 11411212, distinct tokens: 305734\n",
      "ngrams level: 3, total ngrams: 11411210, distinct ngrams: 6785492\n"
     ]
    }
   ],
   "source": [
    "## generate model and text iterations for articles at different depths\n",
    "\n",
    "#define maximum depth and text iterations\n",
    "maxdepth=5\n",
    "maxit=10\n",
    "\n",
    "#article names\n",
    "articles= ['The_Beatles','Led_Zeppelin','The_Rolling_Stones','Black_Sabbath','Fluorine','Chlorine','Bromine','Iodine']\n",
    "\n",
    "for article in articles:\n",
    "    #load article\n",
    "    path = './data/input/'+ article +'_depth_5_spread_10' #path to file\n",
    "    import pickle\n",
    "    with open(path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    wiki_string = [0,0,0,0,0]\n",
    "    #make strings, connect with \\n\n",
    "    wiki_string[0] = \" \\n\".join(list(dataset[1].values()))\n",
    "    length = int(len(Text(\" \\n\".join(wiki_string[0:1])).content.split())/9)    #average length of d=1 summaries\n",
    "    wiki_string[1] = \" \\n\".join(list(dataset[2].values()))\n",
    "    wiki_string[2] = \" \\n\".join(list(dataset[3].values()))\n",
    "    wiki_string[3] = \" \\n\".join(list(dataset[4].values()))\n",
    "    wiki_string[4] = \" \\n\".join(list(dataset[5].values()))\n",
    "    \n",
    "    \n",
    "    for depth in range(1,maxdepth+1):\n",
    "        #connect depths\n",
    "        wiki_text = Text(\" \\n\".join(wiki_string[0:depth]))\n",
    "        \n",
    "        #generate model\n",
    "        chain_model = Chain(wiki_text, n=3)\n",
    "        chain_model.tokens_info()\n",
    "        chain_model.ngrams_info()\n",
    "\n",
    "        #generate 10 texts\n",
    "        for it in range(maxit):\n",
    "            while True:\n",
    "                try:\n",
    "                    output=chain_model.generate_sequence('', int(length)) # '' means to use random prefix\n",
    "                    #outpath with band,depth,iteration\n",
    "                    outpath= './data/output/'+article+'_depth_'+str(depth)+'_'+str(it)+'.txt'\n",
    "                    with open(outpath, 'w', encoding=\"utf-8\") as f:\n",
    "                        f.write(output)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "        #generate_sequence sometimes gives an error, repeats if error\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create reference text file from dataset pickle \n",
    "\n",
    "#choose article name\n",
    "articles= ['The_Beatles','Led_Zeppelin','The_Rolling_Stones','Black_Sabbath','Fluorine','Chlorine','Bromine','Iodine']\n",
    "for article in articles:\n",
    "    path = './data/input/'+ article +'_Depth_5_spread_10' #path to file\n",
    "    import pickle\n",
    "    with open(path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    ref_string= \" \\n\".join(list(dataset[0].values()))\n",
    "    \n",
    "    #save reference text\n",
    "    refpath= './data/output/'+article+'_ref.txt'\n",
    "    with open(refpath, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(ref_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
